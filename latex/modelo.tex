\subsection{Rankings de Páginas Web}

Hoy en día la cantidad de páginas web en todo el mundo asciende a una cantidad de
4.79 billones (sólo las indexadas). Es por esta razón que los buscadores
(Search Engines) cumplen un rol tan importante en el uso diario de internet desde
hace muchos años.

Dichos buscadores nos permiten realizar búsquedas de páginas web mediante distintos
criterios, facilitando el acceso a la información.

Un posible criterio (bastante utilizado) es considerar que las páginas web populares
son las más buscadas. De esta forma, el buscador puede ofrecernos en orden descendiente
de popularidad los resultados obtenidos, esperando que encontremos más rápido lo
 que buscamos.

 Siguiendo esta intuición, se han elaborado diferentes algoritmos para ``rankear''
 las páginas web. A continuación presentaremos el famoso método PageRank, utilizado
 por Google.

\subsubsection{PageRank}\label{PageRank}

El algoritmo de PageRank\cite{Bryan2006} se define para un conjunto de
páginas Web $ = \{ 1,\dots, n \}$ de forma tal de asignar a cada una de ellas un
 puntaje que determine la importancia relativa de la página respecto de las demás.

Llamemos $x_j$ al puntaje asignado a la página $j\in\ Web$, que es lo que buscamos
calcular.

Ahora, un link saliente de la página $j$ a la página $i$ puede significar que $i$
 es una página importante. Pero bien podría ser que $j$ sea una página muy poco
 importante, por lo que deberíamos ponderar sus links salientes para decidir
 la importancia de las páginas a las que apunta.

Luego, vamos a considerar que la importancia de la página $i$ obtenida mediante el
link de $j$ es proporcional a la importancia de $j$ e inversamente proporcional
al grado de $j$. Entonces, si $L_k \in\ Web$ es el conjunto de páginas web que
apuntan a la página $k$:

\begin{equation} \label{eq:puntajes_page_rank}
    x_k = \sum\limits_{j \in\ L_k}{\frac{x_j}{n_j}},\quad k=1,\dots,n
\end{equation}

En este algoritmo, hallaremos los $x_k$ modelando el problema como una cadena de
 Markov, a la cual llamaremos Matriz de Transición, y que construiremos de la siguiente forma:

\begin{enumerate}
	\item Sea $G$ el grafo de la Web, dónde cada vértice es una página web y un
		eje de $v$ a $u$ significa que la página $v$ tiene link saliente hacia $u$.
	\item Luego, sea $W\in\{0,1\}^{n \times n}$ la \textit{matriz de conectividad} de $G$, tal
        que la celda $\{i,j\}$ tiene un 1 si hay un link saliente de la $j$-ésima página
        a la $i$-ésima página (los \textit{autolinks} son ignorados, o lo que es lo mismo,
        $\forall\ 1\leq i\leq n\ W_{i,i}=0$).
	\item Si definimos $n_j = \sum\limits_{i=1}^{n}{W_{i,j}}$ como el grado de $j$
        (la cantidad de links salientes), entonces podemos definir la matriz
        $P\in\mathbb{R}^{n \times n}$, tal que la $P_{i,j} = 1/n_{j} * W_{i,j} $, y
        $P$ es estocástica por columnas.

        Además, nótese que resolver el sistema dado por \ref{eq:puntajes_page_rank}
        es equivalente a encontrar un $x\in\mathbb{R}^n$ tal que $Px=x$.
        Es decir, encontrar el autovector asociado al autovalor 1 de $P$ tal que
        $x_i > 0$ y $\sum\limits_{i=1}^{n}{x_i} = 1$.
    \item Ahora, puede pasar que para algún $j$, $n_j = 0$ lo que indicaría que
        la página $j$ no tiene ningún link saliente. Para remediar
        estos casos, vamos a modificar $P$ utilizando la idea del  \textit{navegante
        aleatorio}, de forma tal que para un $j$ sin links salientes, la probabilidad
        de que el navegante salte a cualquier otra página $i$ es $1/n$.

        Entonces, $P_{1} = P + D$, dónde $D = vd^{t}$, $d\in\{0,1\}^{n}$ tal que
        $d_j = 1$ si $n_j = 0$, y $d_j = 0$ en caso contrario, y $v\in\mathbb{R}^n$
         tal que $v_j = 1/n$.
    \item Por lo tanto, $P_{1}$ es estocástica por columnas, pero puede que no sea regular.
        Para que sí lo sea, extendemos el concepto anterior a todas las páginas
        (fenómeno de \textit{teletransportación}).

        Luego, $P_{2} = c*P_{1} + (1-c)*E$, donde $\forall\ 1\leq i,j\leq n,\ E_{i,j} = 1/n$,
        y $c\in(0,1)$. Llamamos a $c$ coeficiente de teletransportación.

        Lo que nos queda es un matriz $P_{2}$ estocástica por columnas y $\forall\ 1\leq i,j\leq n,\ (P_{2})_{i,j} > 0$.
\end{enumerate}

Una vez que tenemos la Matriz de Transición, generaremos el puntaje para cada página
 buscando el autovector $w$ del autovalor 1 de $P_{2}$, tal que $P_{2}w = w$ y
$w$ sea un vector de probabilidades (normalizado con norma 1).

Es decir, generar el ranking de páginas equivale a aplicar el Método de la Potencia
a la matriz $P_{2}$ y una vez hallado el $w$ mencionado, ordenar los puntajes de
mayor a menor:

\begin{equation} \label{eq:ranking_page_rank}
    ranking = \{ p_1, \dots, p_n \}, \text{donde } \forall\ i=1,\dots,n-1,\ w_{p_{i}} \geq w_{p_{i+1}}
\end{equation}

Falta ver que podemos utilizar el Método de la Potencia con $P_2$. Pero como $P_2$ es una matriz de transición regular, entonces se cumple que:
\begin{itemize}
  \item 1 es un autovalor de $P_2$.
  \item Hay un único vector de probabilidades que es el autovector asociado al autovalor 1.
  \item Los demás autovalores cumplen:
 $$|\lambda_1| > |\lambda_2| \geq \dots \geq |\lambda_n|$$
\end{itemize}

Las condiciones que debe cumplir una matriz $A\in\mathbb{R}^{n \times n}$, con autovalores $\lambda_1,\lambda_2,...,\lambda_n$, y $v_1,v_2,...,v_n$ los autovalores asociados, para poder aplicar el método son:

$$|\lambda_1| > |\lambda_2| \geq \dots \geq |\lambda_n|$$
$$v_1, v_2, \dots, v_n \text{ son autovectores l.i.}$$

Por lo visto, $P_2$ cumple los requisitos necesarios para utilizar el Método de la Potencia.
Explicaremos en detalle el Método de la Potencia en la sección \ref{metodo_potencia}.

\subsubsection{PageRank con matriz esparza}

Partiendo del hecho de que, aproximadamente, cada pagina Web tiene 7 links salientes\cite{Kamvar2003} surge la posibilidad de optimizar la estructura que representa el Grafo dirigido de la Web, debido a la baja densidad de la matriz de conectividad, $W$, asociada a dicho Grafo.
Esta optimización no es solo una posibilidad, sino una necesidad dado que la cantidad de páginas indexadas tiene un requerimiento prohibitivo en términos de memoria si la representamos, ingenuamente, como un vector de vectores.
Nuestra hipótesis es que, utilizando una estructura de datos optimizada, no solo obtendremos mejoras en términos de complejidad espacial, sino también en términos de complejidad temporal.

A partir de este análisis, nos propusimos analizar tres estructuras de datos que representan matrices esparsas:
\begin{enumerate}
  \item \textit{Dictionary of Keys (dok)}
  \item \textit{Compressed Sparse Row (CSR)}
  \item \textit{Compressed Sparse Column (CSC)}
\end{enumerate}

Sea $A \in \mathbb{R}^{n \times n}$ una matriz esparza, y sea $m$ es la cantidad de valores distintos de 0 de $A$.
\paragraph{1. Dictionary of Keys (dok):}
Esta estructura esta representada como un vector de diccionarios, en donde cada posición, $i$, del vector puede considerarse como una fila, y la key del diccionario contenido dentro de esa posición representa una columna, $j$, siendo el valor asociado a esa key, en el diccionario, el valor en la matriz contenido en esa fila y columna ($A_{ij}$). Solo aquellas $A_{ij}$ que tienen valores distintos de 0 tienen una key en los diccionarios.
\newline
\newline
Veamos un ejemplo. Sea $B \in \mathbb{R}^{4 \times 4}$ la siguiente matriz:
\[
  B = \left(\begin{array}{*5{c}}
    10 & 0 & 0 & -2\\
    3  & 9 & 0 & 0 \\
    0  & 7 & 8 & 0 \\
    3  & 0 & 4 & 5
  \end{array}\right)
\]
El dictionary of keys que representa a $B$ es:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
        \{[1;10], [4;-2]\} & \{[1;3], [2;9]\} & \{[2;7], [3;8]\} & \{[1;3], [3;4], [4;5]\} \\
        \hline
	\end{tabular}
\end{table}

Esta claro que intercambiando columnas por filas, como posiciones del vector contenedor, y filas por columnas como keys, representamos la misma matriz.
\newline
\newline
Las ventajas de usar esta estructura de datos son las siguientes:
\begin{enumerate}
  \item Facilidad para crearse de forma dinámicamente, es decir, si a priori no conocemos la forma de la matriz a representar.
  \item La operación matriz esparza por vector es sencilla.
\end{enumerate}
Por otro lado, su principal desventaja es que:
\begin{enumerate}
  \item Las operaciones entre matrices esparsas, sea la suma o la multiplicación, son engorrosas.
\end{enumerate}

\paragraph{2. Compressed Sparse Row (CSR):}

Esta estructura utiliza tres vectores, dos de tamaño $m$, llamémosles \textit{val} y \textit{col_ind} respectivamente, y uno de tamaño $n+1$, llamémosle \textit{row_ptr}.
\begin{itemize}
    \item En \textit{val} guardamos los valores de $A$ distintos de 0, recorriendo $A$ por filas, es decir fijando las filas y avanzado por las columnas.
    \item En la posición $k$ de \textit{col_ind} guardamos el índice $j$ del valor $A_{ij}$ contenido en la posición $k$ del vector \textit{val}.
    \item Por ultimo en \textit{row_ptr} guardamos índices del vector \textit{val} en los cuales se encuentran el primera valor de cada fila, de tal
     forma que para la fila $i$, el valor $k$ contenido en \textit{row_ptr[i]}, nos dice que \textit{val[k]} corresponde al primer valor de la fila
      $i$ y el valor $k'$ contenido en \textit{row_ptr[i+1]}, nos dice que \textit{val[k'-1]} corresponde al ultimo valor de la fila $i$.
      Vale la pena aclarar que si $k = k'$ entonces la fila no tiene valores distintos de 0.
\end{itemize}

Utilicemos $B$ como la matriz del punto anterior:
\[
  B = \left(\begin{array}{*5{c}}
    10 & 0 & 0 & -2\\
    3  & 9 & 0 & 0 \\
    0  & 7 & 8 & 0 \\
    3  & 0 & 4 & 5
  \end{array}\right)
\]

La misma seria representado de la siguiente manera:
\begin{itemize}
  \item \textit{val:}
\end{itemize}
\begin{center}
\begin{tikzpicture}
\tikzset{square matrix/.style={
    matrix of nodes,
    column sep=-\pgflinewidth, row sep=-\pgflinewidth,
    nodes={draw,
      minimum height=#1,
      anchor=center,
      text width=#1,
      align=center,
      inner sep=0pt
    },
  },
  square matrix/.default=0.8cm
}

\matrix[square matrix]
{
10 & -2 & 3 & 9 & 7 & 8 & 3 & 4 & 5 \\
};
\end{tikzpicture}
\end{center}

\begin{center}
\begin{itemize}
  \item \textit{col_ind:}
\end{itemize}
\begin{tikzpicture}

\tikzset{square matrix/.style={
    matrix of nodes,
    column sep=-\pgflinewidth, row sep=-\pgflinewidth,
    nodes={draw,
      minimum height=#1,
      anchor=center,
      text width=#1,
      align=center,
      inner sep=0pt
    },
  },
  square matrix/.default=0.8cm
}

\matrix[square matrix]
{
1 & 4 & 1 & 2 & 2 & 3 & 1 & 3 & 4 \\
};
\end{tikzpicture}
\end{center}

\begin{center}
\begin{itemize}
  \item \textit{row_ptr:}
\end{itemize}

\begin{tikzpicture}

\tikzset{square matrix/.style={
    matrix of nodes,
    column sep=-\pgflinewidth, row sep=-\pgflinewidth,
    nodes={draw,
      minimum height=#1,
      anchor=center,
      text width=#1,
      align=center,
      inner sep=0pt
    },
  },
  square matrix/.default=0.8cm
}

\matrix[square matrix]
{
1 & 3 & 5 & 7 & 10 \\
};
\end{tikzpicture}
\end{center}

Las ventajas de usar esta estructura de datos son las siguientes:
\begin{enumerate}
  \item Eficiente a la hora de realizar operaciones aritméticas entre matrices esparsas (suma, multiplicación).
  \item La operación matriz esparza por vector es sencilla.
\end{enumerate}
Por otro lado, sus desventajas son:
\begin{enumerate}
  \item Dificultad a la hora de crearse de forma dinámica, ya que debemos recalcular/redimensionar los tamaños de los vectores.
  \item Cambios en la esparcidad son costosos, misma razón que el punto anterior.
\end{enumerate}

\paragraph{3. Compressed Sparse Column (CSC):}

Muy parecida a la \textit{Compressed Sparse Row (CSR)}.
 También utiliza tres vectores, dos de tamaño $m$, llamémosles \textit{val} y \textit{row_ind} respectivamente, y uno de tamaño $n+1$, llamémosle \textit{col_ptr}.

 \begin{itemize}
     \item En \textit{val} guardamos los valores de $A$ distintos de 0, recorriendo $A$ por columnas, es decir fijando las columnas y avanzado por las filas.
     \item En la posición $k$ de \textit{row_ind} guardamos el índice $i$ del valor $A_{ij}$ contenido en la posición $k$ del vector \textit{val}.
     \item Por ultimo en \textit{col_ptr} guardamos índices del vector \textit{val} en los cuales se encuentran el primera valor de cada columna, de tal forma que para la columna $j$, el valor $k$ contenido en \textit{col_ptr[j]}, nos dice que \textit{val[k]} corresponde al primer valor de la fila $j$ y el valor $k'$ contenido en \textit{col_ptr[j+1]}, nos dice que \textit{val[k'-1]} corresponde al ultimo valor de la col $j$. Vale la pena aclarar que si $k = k'$ entonces la columna no tiene valores distintos de 0.
 \end{itemize}

Nuevamente utilicemos $B$ como ejemplo:
\[
  B = \left(\begin{array}{*5{c}}
    10 & 0 & 0 & -2\\
    3  & 9 & 0 & 0 \\
    0  & 7 & 8 & 0 \\
    3  & 0 & 4 & 5
  \end{array}\right)
\]

La misma seria representado de la siguiente manera:
\begin{itemize}
  \item \textit{val:}
\end{itemize}

\begin{center}
\begin{tikzpicture}
\tikzset{square matrix/.style={
    matrix of nodes,
    column sep=-\pgflinewidth, row sep=-\pgflinewidth,
    nodes={draw,
      minimum height=#1,
      anchor=center,
      text width=#1,
      align=center,
      inner sep=0pt
    },
  },
  square matrix/.default=0.8cm
}

\matrix[square matrix]
{
10 & 3 & 3 & 9 & 7 & 8 & 4 & -2 & 5 \\
};
\end{tikzpicture}
\end{center}

\begin{center}
\begin{itemize}
  \item \textit{row_ind:}
\end{itemize}
\begin{tikzpicture}

\tikzset{square matrix/.style={
    matrix of nodes,
    column sep=-\pgflinewidth, row sep=-\pgflinewidth,
    nodes={draw,
      minimum height=#1,
      anchor=center,
      text width=#1,
      align=center,
      inner sep=0pt
    },
  },
  square matrix/.default=0.8cm
}

\matrix[square matrix]
{
1 & 2 & 4 & 2 & 3 & 3 & 4 & 1 & 4 \\
};
\end{tikzpicture}
\end{center}

\begin{center}
\begin{itemize}
  \item \textit{col_ptr:}
\end{itemize}

\begin{tikzpicture}

\tikzset{square matrix/.style={
    matrix of nodes,
    column sep=-\pgflinewidth, row sep=-\pgflinewidth,
    nodes={draw,
      minimum height=#1,
      anchor=center,
      text width=#1,
      align=center,
      inner sep=0pt
    },
  },
  square matrix/.default=0.8cm
}

\matrix[square matrix]
{
1 & 4 & 6 & 8 & 10 \\
};
\end{tikzpicture}
\end{center}

Las ventajas de usar esta estructura de datos son las siguientes:
\begin{enumerate}
  \item Eficiente a la hora de realizar operaciones aritméticas entre matrices esparsas (suma, multiplicación).
  \item La operación matriz esparza por vector es sencilla.
\end{enumerate}
Por otro lado, sus desventajas son:
\begin{enumerate}
  \item Dificultad a la hora de crearse de forma dinámica, ya que debemos recalcular/redimensionar los tamaños de los vectores.
  \item Cambios en la esparcidad son costosos, misma razón que el punto anterior.
\end{enumerate}

\paragraph{Contexto de Uso}

Antes de definir que estructura de datos vamos a utilizar para representar matrices esparsas, debemos definir el contexto de uso de la misma.
Sabemos que el algoritmo de PageRank utiliza el Método de la Potencia, usando la matriz $P_2$ definida previamente. Sin embargo la $P_2$ no es esparza, inclusive todos sus valores son distintos de 0, por lo cual utilizar la implementación del Page Rank descripta en la Sección \ref{PageRank} no es posible .

Para poder hacer frente a esta dificultad, utilizamos el Algoritmo 1 propuesto por Kamvar et al.\cite{Kamvar2003} para calcular $x^{(k+1)} = P_2x^{k}$, cuyos pasos son los siguientes:
\begin{enumerate}
    \item $y = cPx$
    \item $w = ||x||_1 - ||y||_1$
    \item $y = y + wv$
\end{enumerate}

donde $c$, $P$ y $v$ corresponden al escalar, la matriz y el vector descriptos en la Sección \ref{PageRank}.

Tenemos que ver que el algoritmo descripto calcula efectivamente $x^{(k+1)} = P_2x^{k}$:
\begin{proposition}
    El Algoritmo 1 calcula $x^{(k+1)} = P_2x^{(k)}$
\end{proposition}
\begin{proof}
Por definición sabemos que
  \begin{equation*}
            \begin{aligned}
              x^{(k+1)} &= P_2x^{(k)} \\
               &= (cP_1 +(1-c)E)x^{(k)} \\
               &= (cP + cD +(1-c)E)x^{(k)} \\
               &= (cP + cD +(1-c)E)x^{(k)} \\
               &= (cP + cvd^{t} +(1-c)v1^{t})x^{(k)} \\
               &= cPx^{(k)} + cvd^{t}x^{(k)} +(1-c)v1^{t}x^{(k)} \\
            \end{aligned}
\end{equation*}
Por otro lado, el Algoritmo 1 nos dice que:
\begin{equation*}
          \begin{aligned}
            x^{(k+1)} &= cPx^{(k)} + (||x^{(k)}||_1 - ||cPx^{(k)}||_1)v  \\
          \end{aligned}
\end{equation*}
Luego basta ver que:
\begin{equation*}
          \begin{aligned}
            cPx^{(k)} + (||x^{(k)}||_1 + ||cPx^{(k)}||_1)v &= cPx^{(k)} + cvd^{t}x^{(k)} +(1-c)v1^{t}x^{(k)}  \\
            (||x^{(k)}||_1 - ||cPx^{(k)}||_1)v &= cvd^{t}x^{(k)} +(1-c)v1^{t}x^{(k)}  \\
            ||x^{(k)}||_1v - ||cPx^{(k)}||_1v &= cvd^{t}x^{(k)} +v1^{t}x^{(k)} - cv1^{t}x^{(k)}  \\
          \end{aligned}
\end{equation*}
Sabemos que $\forall\ 1\leq i\leq n\ x^{(k)}_{i} \geq 0$, ya que $x$ es un vector de probabilidades. Entonces $||x^{(k)}||_1$ = $\sum\limits_{i=1}^{n}{x^{(k)}_{i}}$. También sabemos que $1^{t}x^{(k)} = \sum\limits_{i=1}^{n}{1x^{(k)}_{i}}$, por lo tanto
$||x^{(k)}||_1 = 1^{t}x^{(k)}$.
\newline
Por otro lado sabemos que $0\leq c\leq 1$, lo que implica que $|c| = c$.
\newline
Teniendo en cuenta esta información, la igualdad nos queda de la siguiente forma:
\begin{equation*}
          \begin{aligned}
            v1^{t}x^{(k)} - c||Px^{(k)}||_1v &= cvd^{t}x^{(k)} + v1^{t}x^{(k)} - cv1^{t}x^{(k)}  \\
            - c||Px^{(k)}||_1v &= cvd^{t}x^{(k)} - cv1^{t}x^{(k)}  \\
          \end{aligned}
\end{equation*}
Tomando como factor común $v$ y $c$:
\begin{equation*}
          \begin{aligned}
            - c||Px^{(k)}||_1v &= cv(d^{t}x^{(k)} - 1^{t}x^{(k)})  \\
            - ||Px^{(k)}||_1 &= d^{t}x^{(k)} - 1^{t}x^{(k)}  \\
            - ||Px^{(k)}||_1 &= (d^{t} - 1^{t})x^{(k)}  \\
          \end{aligned}
\end{equation*}

Sabemos, por definición de $d$, que $d^{t}$ solo tiene $1$ y $0$ como valores, por lo tanto al realizar $(d^{t} - 1^{t})$ obtendremos un vector fila,
llamémosle $z$, cuyos valores serán $0$ o $-1$. Lo que implica que $z_j = 0$ cuando $d^{t}_j = 1$, situación que se da cuando, para ese
$j$, $\sum_{i=1}^{n}{P_{ij}} = 0$. Caso contrario $z_j = -1$ cuando $d^{t}_j = 0$, situación que se da cuando, para ese $j$,
 $\sum_{i=1}^{n}{P_{ij}} = 1$. Por lo tanto al multiplicar $zx^{(k)}$ obtendremos un escalar que sera la suma de los opuestos de
 los $x^{(k)}_j$ que cumplan que, para ese $j$, $\sum_{i=1}^{n}{P_{ij}} = 1$, ya que los otros serán multiplicados por $0$.

Por otro lado, partiendo del hecho de que $P$ es estocástica por columnas, sabemos que al realizar $Px^{(k)}$, aquellas columnas, $j$, que
cumplan que $\sum_{i=1}^{n}{P_{ij}} = 0$, forzaran a que los $x^{(k)}_j$ no sean parte de ninguna componente del vector resultante,
ya que en todas las multiplicaciones de filas de $P$ por el vector $x^{(k)}$, estos $x^{(k)}_j$ serán multiplicados por $0$. Caso contrario
cuando las columnas, $j$, cumplan que $\sum_{i=1}^{n}{P_{ij}} = 1$, estas forzaran a que la suma de todos los $x^{(k)}_j$ presentes
en las distintas componentes del vector resultante sea igual a $x^{(k)}_j$. Precisamente al aplicar $||.||_1$ al vector $Px^{(k)}$, el
resultado sera un escalar que sera la suma de todas los $x^{(k)}_j$ que cumplan la condición antes descripta. Por ultimo si al resultado
de aplicar $||.||_1$ lo multiplicamos por $-1$ obtenemos un escalar que sera la suma de los opuestos de los $x^{(k)}_j$ que cumplan que,
para ese $j$, $\sum_{i=1}^{n}{P_{ij}} = 1$.

Por todo lo expuesto anteriormente, podemos concluir que $- ||Px^{(k)}||_1 = (d^{t} - 1^{t})x^{(k)}$, lo que implica que El Algoritmo 1
calcula $x^{(k+1)} = P_2x^{(k)}$.
\end{proof}

Luego, sabemos que la única operación que vamos a realizar con la matriz esparza es multiplicarla por un vector. También sabemos que la
construcción de la matriz es de forma dinámica, ya que vamos leyendo los links salientes de cada pagina de forma secuencial.

A partir de esta información concluimos que la estructura de datos que mas se adecua a nuestras necesidades es el \textit{Dictionary
of Keys (dok)}, en donde las posiciones del vector representan las filas y las keys de los diccionarios las columnas.

Esta elección se debe al hecho de que podemos actualizar la matriz de forma sencilla a medida de que vamos leyendo los datos de entrada, y a su vez
 la multiplicación por un vector se realiza de forma sencilla, véase Sección \ref{imp_esparza}.

\newpage
\subsection{Rankings en competencias deportivas}\label{rankings_deportivos}

Elegir un sistema de puntos que sea justo para todos los participantes en un
deporte no es una tarea sencilla. Existen muchos factores que afectan el
resultado de una competencia como lo pueden ser el orden en el que deben
competir entre si los equipos, generando desbalances respecto las capacidades de
cada uno. Es por esto que a continuación presentaremos el modelo GeM\cite{Govan2008}
 que busca modelar los resultados de forma tal que estos
factores impacten lo menos posible en el posicionamiento final de la tabla de
puntajes.

Con el fin de experimentar con distintos modelos, a lo largo del informe
trabajaremos sobre los resultados del Torneo de Primera División
2015\footnote{Campeonato de Primera División 2015, \textit{Julio H. Grondona} \\
\url{http://www.afa.org.ar/html/9/estadisticas-de-primera-division}}, donde
utilizaremos el sistema de ranking estándar de la AFA como punto de comparación.

\subsubsection{Generalized Markov chains Method (GeM)}
\label{sec:gem_model}

\subsubsection*{Definición del método}

El método GeM es el resultado de tomar el algoritmo PageRank y mediante pequeñas
modificaciones utilizar su potencial para establecer un ranking de equipos.
Análogo a PageRank, los equipos pasan a formar parte de un grafo dirigido
con pesos, donde cada nodo representa un equipo y los pesos de cada arista
reflejan el resultado de los partidos jugados entre los vértices conectados.

~

Formalmente, el modelado se realiza de la siguiente manera:
\begin{enumerate}
	\item Representamos el torneo como un grafo con pesos dirigidos de $n$
	nodos, donde $n$ es igual a la cantidad de equipos que participan. Cada
	equipo tiene su respectivo nodo y las aristas contienen como peso la
	diferencia positiva entre los nodos conectados.

	\item Definimos la matriz de adyacencia $A \in \mathbb{R}^{n \times n}$.
		\begin{equation*}
			A_{ij} =
				\begin{cases}
					w_{ij} & \text{si el equipo $i$ perdió contra $j$}\\
					0 & \text{caso contrario}
				\end{cases}
		\end{equation*}
		Donde $w_{ij}$ es la suma total de diferencia positiva de puntaje sobre todos
		los partidos en los que $i$ perdió contra $j$.

	\item Definimos la matriz $H \in \mathbb{R}^{n \times n}$.
		\begin{equation*}
			H_{ij} =
				\begin{cases}
					A_{ij}/\sum_{k = 1}^{n}A_{ik} & \text{si hay un link de $i$ a $j$}\\
					0 & \text{caso contrario}
				\end{cases}
		\end{equation*}

	\item Definimos la matriz GeM, $G \in \mathbb{R}^{n \times n}$ con $u, v, a, e \in \mathbb{R}^n$ y $c \in \mathbb{R}$.
		\begin{gather*}
			G = c(H + au^{t}) + (1 - c)ev^{t} \\
			\sum_{k = 1}^{n}v_{k} = 1 \qquad \sum_{k = 1}^{n}u_{k} = 1 \qquad \forall_{i = 1..n} e_{i} = 1 \\
			0 \leq c \leq 1 \qquad
			a_{i} =
			\begin{cases}
				1 & \text{si la fila $i$ de $H$ es un vector nulo} \\
				0 & \text{caso contrario}
			\end{cases}
		\end{gather*}

	\item Por último tenemos que el ranking de los equipos estará definido por
		el vector $\pi \in \mathbb{R}^n$ tal que
		\begin{gather*}
			\pi^{t} = \pi^{t}G \\
			\text{o si tomamos la transpuesta en ambos lados} \\
			G^{t}\pi = \pi
		\end{gather*}
\end{enumerate}

De esta forma al igual que con PageRank, calculando el autovector $\pi$
obtenemos nuestro ranking. Este modelo permite cierta flexibilidad a partir del
$u$, $v$ y $c$ que tomemos.

El vector de probabilidad $u$ se aplicará en el caso de que un equipo se encuentre invicto, esto
es el equivalente a que en PageRank un sitio no tenga ningún link entrante, por
lo tanto su tratamiento es el mismo, se le asigna a la fila correspondiente el
vector con las probabilidades de saltar a otro nodo. Por lo tanto, el vector $u$
nos permite definir con qué probabilidades un equipo invicto perdería contra el
resto de los participantes. En el caso de PageRank, este es un vector de
distribución uniforme, donde es igual la posibilidad de saltar a cualquiera de
los otros nodos, una posible alternativa sería definirlo como un vector cuyas
probabilidades se basen en algún ranking anterior.

El vector de probabilidad $v$, nos da otro tipo de personalización que es
la del \textit{navegante aleatorio}. Esta es la probabilidad de que un equipo
cualquiera independientemente de los resultados registrados, pierda contra el
resto de los equipos. En PageRank, esto lo veíamos como la posibilidad de que
estando navegando el grafo, uno se \textit{teletransportará} a otro nodo
independientemente de las conexiones de los mismos. Este vector por defecto
también suele tomar el valor de la distribución uniforme.

Por último tenemos nuestro valor $c$ que actúa como un factor de amortiguación
donde lo que se modifica es cuánto afecta el \textit{navegante aleatorio} al
resultado final, donde con $c = 0$, únicamente influye el \textit{navegante
aleatorio} y con $c = 1$ se elimina el efecto del mismo.

\subsubsection*{Modelado del empate}

Una particularidad de este sistema que se puede observar en la definición del
mismo es que no contempla los partidos donde hubo empate. Un empate equivale
a que no exista un perdedor y por ende no se modifica el peso de ningún nodo.
Para deportes donde el empate no es algo frecuente esto no sería un problema,
pero si tomamos como ejemplo el fútbol, donde los empates son algo mucho más
común, el ignorar estos partidos afecta notablemente el ranking.

Podemos tomar como ejemplo los Torneos Argentinos de Primera División y
observar el porcentaje de partidos empatados sobre el total que se jugaron.

\begin{table}[H]
	\centering
	\begin{tabular}{|cccc|}
		\hline
		Campeonato de Primera División & Partidos & Empates &  Empates/Partidos \\ \hline
		2015 & 390 & 126 & 0.32 \\ \hline
		2014 & 190 & 45 & 0.23 \\ \hline
		2013/14 & 380 & 117 & 0.31 \\ \hline
	\end{tabular}
	\caption*{Relación entre partidos jugados y empates totales}
\end{table}

Como podemos ver, la proporción de partidos empatados no es menor, con lo cual
analizaremos cómo impactaría esto en el método GeM. Como mencionamos en la
definición del modelo, los pesos de las aristas en el grafo dependen en parte de la
diferencia de puntaje con la que gana un equipo sobre otro.

Al encontrarnos con un empate no existe diferencia alguna, por lo tanto no
implica ningún cambio en el sistema. Esto podría desfavorecer a los equipos que
más empates tuvieran en la temporada, ya que sólo los partidos donde hubieran
ganado o perdido afectarían su lugar en la tabla de puntajes. Además podría
suceder que un equipo perdiese contra un contrincante que no tuviera mucho peso
pero empatara contra un puntero, y esto debería verse reflejado ya que de otra
manera, sólo quedaría registrado su partido perdido.

En base a esto, proponemos dos formas alternativas de modelar el empate:

\begin{itemize}
    \item Una posible forma de representar los empates aprovechando la estructura del
        modelo actual sería que cuando se produce uno, reflejar en el grafo el equivalente a
        que ambos equipos hubieran perdido entre si, asignando como diferencia de
        puntaje el valor con el que empataron. De esta forma no solo incluiríamos este
        escenario al cálculo del ranking final, si no que además el puntaje con el que
        empataron tendría peso.
    \item Otra posibilidad sería que en caso de empate, se utilizaran otras métricas
        del deporte para definir el ganador, al cual luego se le podría asignar
        el puntaje mínimo para ganar el partido. Por ejemplo, si tomamos el caso
        del fútbol, en caso de empate podríamos definir el ganador como el equipo
        que tuvo mayor posesión del balón durante el partido, o el equipo que tuvo
        menor cantidad de tarjetas, o una combinación de varios factores. Una vez
        determinado el ganador, el score asignado sería 1-0.
        De esta forma, rompemos el empate refinando el criterio de ganador, y como
        asignamos el mínimo puntaje para ganar, el puntaje en el ranking que obtiene
        el ganador no es altamente influenciado por el partido en cuestión.
\end{itemize}

\subsubsection{Puntaje AFA}

A modo de comparación con el método GeM, siendo que trabajaremos sobre los
resultados del Torneo de Primera División, utilizaremos también el sistema de
puntajes que propone la
AFA\footnote{\url{http://www.afa.org.ar/upload/reglamento/Reglamento\_PrimeraDivision\_2015.pdf}}.
El mismo es bastante sencillo, donde lo que establece es que todos los equipos
comienzan con puntaje \textbf{0} y se les suma \textbf{1} punto en caso de
empate, \textbf{3} por victoria y \textbf{0} por derrota.

A primera vista, dada que la modalidad de este torneo es de todos contra todos,
donde cada equipo juega una vez contra el resto, el resultado debería reflejar
de forma justa el desempeño de cada participante. Sin embargo, para este torneo
en particular se aplicó una fecha adicional de \textit{clásicos}, donde cada
equipo vuelve a jugar contra su respectivo clásico. Este es un punto donde se
desequilibran un tanto las cosas ya que un participante podría tener asignado
como clásico un puntero, mientras que otro al último en la tabla de posiciones.
Para este sistema de puntuación ambos partidos serían tratados al igual que el
resto del torneo.

\newpage
\subsection{Método de la potencia}\label{metodo_potencia}

El Método de la Potencia es un método iterativo que calcula sucesivas aproximaciones a los autovectores y autovalores de una matriz.

Sea $A\in\mathbb{R}^{n \times n}$ una matriz cuadrada y sean $\lambda_1,\lambda_2,...,\lambda_n$
sus autovalores, con $v_1,v_2,...,v_n$ los autovalores asociados. Para poder aplicar el método a
la matriz $A$, debe valer que:

$$|\lambda_1| > |\lambda_2| \geq \dots \geq |\lambda_n|$$
$$v_1, v_2, \dots, v_n \text{ son autovectores l.i.}$$

A continuación demostraremos como funciona el método.

\begin{proposition}
    Sea $x_0 \in \mathbb{R}^{n}$. Luego, $x_{(k+1)} = \frac{Ax_k}{||Ax_k||} \xrightarrow{k \to \infty} v_1$.
\end{proposition}
\begin{proof}

Como $\{v_1, \dots, v_n\}$ es una base de autovectores l.i., podemos escribir $x_0 = \sum\limits_{j=1}^{n}{\beta_j v_j}$.

Entonces, $Ax = \sum\limits_{j=1}^{n}{\beta_j A v_j} = \sum\limits_{j=1}^{n}{\beta_j \lambda_j v_j}$.

Si multiplicamos una vez más por A tenemos que: $A^2x = \sum\limits_{j=1}^{n}{\beta_j \lambda_j A v_j} = \sum\limits_{j=1}^{n}{\beta_j \lambda_j^2 v_j}$.

Y generalizando:

$$A^kx = \sum\limits_{j=1}^{n}{\beta_j \lambda_j^k v_j}$$

Reescribiendo, tenemos que:
\begin{equation*}
        \begin{aligned}
        A^kx &= \sum\limits_{j=1}^{n}{\beta_j \lambda_j^k v_j} \\
             &= \beta_1 \lambda_1^k v_1 +\sum\limits_{j=2}^{n}{\beta_j \lambda_j^k v_j} \\
             &= \lambda_1^k \left(\beta_1 v_1 +\sum\limits_{j=2}^{n}{\beta_j \frac{\lambda_j^k}{\lambda_1^k} v_j} \right) \\
        \end{aligned}
\end{equation*}

Y es fácil ver que $\lim\limits_{k \to \infty} \sum\limits_{j=2}^{n}{\beta_j \frac{\lambda_j^k}{\lambda_1^k} v_j} = 0$,
ya que $\forall\ 2\leq j\leq n,\ |\lambda_1| > |\lambda_i|$.

Luego, consideremos una función $\phi:\mathbb{R}^{n}\to \mathbb{R}$ tal que:
\begin{itemize}
    \item $\phi(\alpha x) = \alpha \phi(x)$
    \item $\phi(0) = 0$
    \item $\phi$ continua
    \item $\phi(\beta_1 v_1) \neq 0 \iff \beta_1 v_1 \neq 0$
\end{itemize}

Entonces, podemos escribir:
\begin{equation*}
        \begin{aligned}
        \frac{\phi(A^{k+1}x)}{\phi(A^kx)}
            &= \frac{\lambda_1^{k+1} \phi\left(\beta_1 v_1 +\sum\limits_{j=2}^{n}{\beta_j \frac{\lambda_j^{k+1}}{\lambda_1^{k+1}} v_j} \right)}
                {\lambda_1^k \phi\left(\beta_1 v_1 +\sum\limits_{j=2}^{n}{\beta_j \frac{\lambda_j^k}{\lambda_1^k} v_j} \right)}
            &= \lambda_1\frac{\phi\left(\beta_1 v_1 +\sum\limits_{j=2}^{n}{\beta_j \frac{\lambda_j^{k+1}}{\lambda_1^{k+1}} v_j} \right)}
                    {\phi\left(\beta_1 v_1 +\sum\limits_{j=2}^{n}{\beta_j \frac{\lambda_j^k}{\lambda_1^k} v_j} \right)}
        \end{aligned}
\end{equation*}

Y tomando el límite con $k$ tendiendo a infinito:
\begin{equation*}
        \begin{aligned}
        \lim\limits_{k \to \infty} \frac{\phi(A^{k+1}x)}{\phi(A^kx)}
            &= \lambda_1 \lim\limits_{k \to \infty}
                    \frac{\phi\left(\beta_1 v_1 +\sum\limits_{j=2}^{n}{\beta_j \frac{\lambda_j^{k+1}}{\lambda_1^{k+1}} v_j} \right)}
                    {\phi\left(\beta_1 v_1 +\sum\limits_{j=2}^{n}{\beta_j \frac{\lambda_j^k}{\lambda_1^k} v_j} \right)}
            &= \lambda_1 \frac{\phi\left(\beta_1 v_1\right)}{\phi\left(\beta_1 v_1 \right)}
            &= \lambda_1
        \end{aligned}
\end{equation*}

Finalmente,
\begin{equation*}
        \begin{aligned}
          x_{(k+1)}
            &= \frac{Ax_k}{||Ax_k||}
            &= \frac{A^kx}{||A^kx||}
            &= \frac{\lambda_1^k \left(\beta_1 v_1 +\sum\limits_{j=2}^{n}{\beta_j \frac{\lambda_j^k}{\lambda_1^k} v_j} \right)}{||\lambda_1^k \left(\beta_1 v_1 +\sum\limits_{j=2}^{n}{\beta_j \frac{\lambda_j^k}{\lambda_1^k} v_j} \right)||}
        \end{aligned}
\end{equation*}

Ahora, tomando límites:

\begin{equation*}
        \begin{aligned}
        \lim\limits_{k \to \infty} \frac{\lambda_1^k \left(\beta_1 v_1 +\sum\limits_{j=2}^{n}{\beta_j \frac{\lambda_j^k}{\lambda_1^k} v_j} \right)}{||\lambda_1^k \left(\beta_1 v_1 +\sum\limits_{j=2}^{n}{\beta_j \frac{\lambda_j^k}{\lambda_1^k} v_j} \right)||}
        &= \lim\limits_{k \to \infty} \frac{\lambda_1^k \beta_1 v_1}{||\lambda_1^k \beta_1 v_1||}
        &= \pm \frac{v_1}{||v_1||}
        \end{aligned}
\end{equation*}

Lo cual tiende a un autovector asociado al autovalor principal $\lambda_1$. Si podemos garantizar que $\lambda_1^k \beta_1> 0$, entonces podríamos decir que $x_{(k+1)}$ tiende a $\frac{v_1}{||v_1||}$ cuando k tiende al infinito. Además, si el vector $v_1$ se encuentra normalizado, concluimos que $x_{(k+1)}  \xrightarrow{k \to \infty} v_1$.

En la práctica se utiliza $\phi(x)= x_p$, dónde $|x_p| = ||x||_\infty$, que cumple las condiciones pedidas.
\end{proof}
